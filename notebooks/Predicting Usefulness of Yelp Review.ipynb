{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","collapsed_sections":["GLwm_KgePpNr","p8xTWhV66qq9","zqA6XXTgsApX","EvvrA-zgsIGx","jkFdvl12PHfR","LHgQT_cRQKy5"],"mount_file_id":"12BctrAhT6bCPLi_qW-7ucVu-mxnL8bld","authorship_tag":"ABX9TyOXRgVHwxynQCExMxTRB6Q1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"hglQZybbE4zn"},"outputs":[],"source":["# GPU Check\n","import torch\n","print(torch.cuda.device_count())  # Number of GPUs\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oZ10a1i-5mpg"},"outputs":[],"source":["# Mount google drive\n","#from google.colab import drive\n","#drive.mount('/content/drive')"]},{"cell_type":"code","source":["# Import the necessary libraries.\n","import re\n","import string\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from wordcloud import WordCloud\n","\n","# NLTK imports\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","from nltk.stem import WordNetLemmatizer\n","#from nltk.sentiment.vader import SentimentIntensityAnalyzer # Perform VADER Sentiment Analysis\n","\n","# Download NLTK resources\n","nltk.download('punkt')\n","nltk.download('punkt_tab')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')\n","\n","# Sklearn\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","from sklearn.utils import resample"],"metadata":{"id":"EAqDm0bZ50Wx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the dataset\n","review_data = pd.read_csv('/content/drive/MyDrive/capstone_data/data.csv')\n","\n","# Display settings\n","pd.set_option('display.width', 100)\n","pd.set_option('display.max_colwidth', 200)\n","\n","# Inspect the first two rows\n","review_data.head(2)"],"metadata":{"collapsed":true,"id":"J_CblPhi6lHz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# List of columns\n","review_data.columns"],"metadata":{"id":"YI9qUsf450Tg","executionInfo":{"status":"error","timestamp":1744744499631,"user_tz":240,"elapsed":109,"user":{"displayName":"Lola Akinsehinwa","userId":"11671398346519568182"}},"outputId":"cb995777-f7a1-483b-feaa-2066b943ec0d","colab":{"base_uri":"https://localhost:8080/","height":162}},"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'review_data' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-1ae7380e1974>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# List of columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mreview_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'review_data' is not defined"]}]},{"cell_type":"code","source":["# Check information to assess null values and data types\n","review_data.info()"],"metadata":{"id":"gIPxsqK250Kv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data preprocessing"],"metadata":{"id":"GLwm_KgePpNr"}},{"cell_type":"code","source":["# Check for duplicate using the review_id field\n","review_data['review_id'].duplicated().sum()"],"metadata":{"id":"XvjMmtx8q_Qw","executionInfo":{"status":"error","timestamp":1744744505704,"user_tz":240,"elapsed":314,"user":{"displayName":"Lola Akinsehinwa","userId":"11671398346519568182"}},"outputId":"007229bf-a03a-46e4-8832-d914de042e49","colab":{"base_uri":"https://localhost:8080/","height":162}},"execution_count":2,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'review_data' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-73f3be458249>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Check for duplicate using the review_id field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mreview_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduplicated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'review_data' is not defined"]}]},{"cell_type":"code","source":["# Drop duplicates\n","review_data.drop_duplicates(inplace=True)"],"metadata":{"id":"CdebUXN3rXpF","executionInfo":{"status":"aborted","timestamp":1744744506215,"user_tz":240,"elapsed":685,"user":{"displayName":"Lola Akinsehinwa","userId":"11671398346519568182"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Count total duplicates considering all columns\n","total_duplicates = review_data.duplicated(keep=False).sum()\n","print(f'Total duplicates considering all columns: {total_duplicates}')\n","\n","# Show duplicates if any\n","review_data[review_data.duplicated(keep=False)].sort_values(by=list(review_data.columns)).head(10)"],"metadata":{"id":"x_DYXfnarU3h","executionInfo":{"status":"aborted","timestamp":1744744506305,"user_tz":240,"elapsed":2435,"user":{"displayName":"Lola Akinsehinwa","userId":"11671398346519568182"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a string of all punctuation characters, excluding apostrophes\n","punctuation_to_remove = string.punctuation.replace(\"'\", \"\")  # Keep apostrophes\n","\n","# regular expression to find and identify unwanted punctuation characters\n","regex_punctuation = re.compile('[%s]' % re.escape(punctuation_to_remove))\n","\n","# regular expression to identify one or more whitespace characters\n","regex_whitespace = re.compile('\\s+') #remove extra spaces\n","\n","# regular expression to detect newline characters ()\n","regex_newline = re.compile(r'\\r?\\n|\\r')#clean up line breaks in text\n","\n","# regular expression to locate whole numbers (sequences of digits)\n","regex_digits = re.compile(r'\\b\\d+\\b')\n"],"metadata":{"id":"-KfsjtYAsWro","executionInfo":{"status":"error","timestamp":1744744506558,"user_tz":240,"elapsed":143,"user":{"displayName":"Lola Akinsehinwa","userId":"11671398346519568182"}},"outputId":"79f263e1-f354-4aaa-c9c9-e621552eebd9","colab":{"base_uri":"https://localhost:8080/","height":216}},"execution_count":3,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'string' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-dfeb3f2eb078>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create a string of all punctuation characters, excluding apostrophes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpunctuation_to_remove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Keep apostrophes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# regular expression to find and identify unwanted punctuation characters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mregex_punctuation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[%s]'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mescape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpunctuation_to_remove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'string' is not defined"]}]},{"cell_type":"code","source":["# Function for additional cleaning\n","def clean_text(data):\n","    # Convert text to lower case\n","    data = str(data).lower()\n","\n","    # Replace escaped characters (e.g., \\' with ')\n","    data = re.sub(r\"\\\\'\", \"'\", data)\n","\n","    # Remove punctuation (except apostrophes)\n","    data = regex_punctuation.sub('', data)\n","\n","    # Remove extra spaces\n","    data = regex_whitespace.sub(' ', data)\n","\n","    # Remove newline characters\n","    data = regex_newline.sub('', data)\n","\n","    # Remove standalone digits\n","    data = regex_digits.sub('', data)\n","\n","    # Strip leading/trailing spaces\n","    data = data.strip()\n","\n","    return data\n","\n","# Apply the function to the data\n","review_data['cleaned_text'] = review_data['text'].apply(lambda reviewtext: clean_text(reviewtext))"],"metadata":{"id":"rBna6tJT5xPC","executionInfo":{"status":"aborted","timestamp":1744744506806,"user_tz":240,"elapsed":2926,"user":{"displayName":"Lola Akinsehinwa","userId":"11671398346519568182"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get the list of stop words\n","stop_words = set(stopwords.words('english'))\n","#print(stop_words)\n","\n","review_data['cleaned_text'] = review_data['cleaned_text'].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words]))"],"metadata":{"id":"6ybqb-tN5xFI","executionInfo":{"status":"aborted","timestamp":1744744506859,"user_tz":240,"elapsed":2975,"user":{"displayName":"Lola Akinsehinwa","userId":"11671398346519568182"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Lemmatization\n","lemmatizer = WordNetLemmatizer()\n","review_data['cleaned_text'] = review_data['cleaned_text'].apply(lambda x:' '.join([lemmatizer.lemmatize(word) for word in x.split()]))"],"metadata":{"id":"JLJ_9o5b5w3u","executionInfo":{"status":"error","timestamp":1744744507019,"user_tz":240,"elapsed":82,"user":{"displayName":"Lola Akinsehinwa","userId":"11671398346519568182"}},"outputId":"edf2d21c-0812-4825-8b9f-d12c4f84baa1","colab":{"base_uri":"https://localhost:8080/","height":180}},"execution_count":4,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'WordNetLemmatizer' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-37d9f22825af>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Lemmatization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlemmatizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mreview_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cleaned_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreview_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cleaned_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'WordNetLemmatizer' is not defined"]}]},{"cell_type":"code","source":["# New features for additional insights\n","review_data['review_length'] = review_data['text'].apply(len)\n","review_data['word_count'] = review_data['text'].apply(lambda x: len(x.split()))\n","review_data['char_count'] = review_data['text'].apply(lambda x: len(x))\n","review_data['sentence_count'] = review_data['text'].apply(lambda x: len(sent_tokenize(x)))"],"metadata":{"id":"w8ZhGvMt5wuE","executionInfo":{"status":"aborted","timestamp":1744744507076,"user_tz":240,"elapsed":3184,"user":{"displayName":"Lola Akinsehinwa","userId":"11671398346519568182"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# New feature-  business popularity\n","review_data['is_popular_business'] = (review_data['review_count'] > 100).astype(int)"],"metadata":{"id":"wusYF2PBpn5p","executionInfo":{"status":"aborted","timestamp":1744744507225,"user_tz":240,"elapsed":67,"user":{"displayName":"Lola Akinsehinwa","userId":"11671398346519568182"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Categorize the useful field.\n","review_data['label'] = review_data['useful'].apply(lambda x: 0 if x == 0 else 1)"],"metadata":{"id":"TBeeYOeORB0K","executionInfo":{"status":"aborted","timestamp":1744744507288,"user_tz":240,"elapsed":3387,"user":{"displayName":"Lola Akinsehinwa","userId":"11671398346519568182"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import library for resample\n","from sklearn.utils import resample\n","\n","# Define majority and minority class\n","useful_majority = review_data[review_data.label==0]\n","useful_minority = review_data[review_data.label !=0]\n","\n","# undersample majority class\n","useful_majority_undersampled = resample(useful_majority,\n","                                 replace=False,\n","                                 n_samples=12100,\n","                                 random_state=42)\n","\n","# Combine and shuffle the dataset\n","resampled_df = pd.concat([useful_majority_undersampled, useful_minority])\n","resampled_df  = resampled_df .sample(frac=1, random_state=42).reset_index(drop=True)\n","\n","# Display the new class counts\n","resampled_df.label.value_counts()"],"metadata":{"id":"BXK202k3RW05","executionInfo":{"status":"error","timestamp":1744744507535,"user_tz":240,"elapsed":176,"user":{"displayName":"Lola Akinsehinwa","userId":"11671398346519568182"}},"outputId":"70583399-d04e-4c89-b7cf-104d8f001604","colab":{"base_uri":"https://localhost:8080/","height":216}},"execution_count":5,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'review_data' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-517d7b314795>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Define majority and minority class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0museful_majority\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreview_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreview_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0museful_minority\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreview_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreview_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'review_data' is not defined"]}]},{"cell_type":"code","source":["resampled_df.columns"],"metadata":{"id":"nDxrYsGlbDLg","executionInfo":{"status":"aborted","timestamp":1744744507448,"user_tz":240,"elapsed":3539,"user":{"displayName":"Lola Akinsehinwa","userId":"11671398346519568182"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Exploratory Data Analysis"],"metadata":{"id":"p8xTWhV66qq9"}},{"cell_type":"markdown","source":["## Univariate Analysis"],"metadata":{"id":"zqA6XXTgsApX"}},{"cell_type":"code","source":["print('\\nDescriptive Statistics')\n","review_data.describe()"],"metadata":{"id":"eVKM64H-I5lf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate distibution of the label\n","useful_counts = review_data['label'].value_counts()\n","\n","#create a figure\n","plt.figure(figsize=(6, 6))\n","plt.pie( # create pie chart\n","    useful_counts,\n","    labels=['Not Useful', 'Useful'],\n","    colors=['#1f77b4', '#5F9ED1'],\n","    autopct='%1.1f%%',\n","    startangle=90,\n","    wedgeprops={'edgecolor': 'white', 'linewidth': 0.5},\n","    textprops={'fontsize': 12}\n",")\n","\n","# Add title and formatting\n","plt.title('Overall Review Usefulness Distribution', pad=20, fontsize=14)\n","plt.tight_layout()"],"metadata":{"id":"XYGQ2F4aBenc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Review trend over time\n","review_data['date'] = pd.to_datetime(review_data['date']) # get the date(year) from date field\n","review_data.set_index('date', inplace=True)\n","review_data.resample('M')['useful'].sum().plot(figsize=(12,6))\n","plt.title('Useful Votes Over Time') # title\n","plt.xlabel('Date') #x label\n","plt.ylabel('Useful Votes') #y label\n","plt.show()\n"],"metadata":{"id":"VEC6HJZMfaVl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Boxplot to show useful votes\n","plt.figure(figsize=(6,4)) # create figure\n","sns.boxplot(x=review_data['useful'], showfliers=False, width=0.3, color='skyblue')\n","sns.stripplot(x=review_data['useful'], color='blue', alpha=0.3, size=3, jitter=True)\n","plt.xlim(-1, 30)\n","plt.xlabel('Useful Votes')\n","plt.title('Spread of \"Useful\" Votes')"],"metadata":{"id":"YrMbj74N5x9y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Top 10 business with the most reviews\n","plt.figure(figsize=(4, 4))\n","review_data['name'].value_counts().head().plot(kind = 'bar', figsize = (5,4), color = '#1F77B4')\n","plt.title('Top 5 Business with the Most Reviews')\n","plt.xlabel('Business name')\n","plt.ylabel('Count')\n","plt.show()"],"metadata":{"id":"vTdqlgIR6MjM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get top 5 business names\n","top_5 = review_data['name'].value_counts().head(5).index\n","\n","# Filter data for the top 5\n","top_reviews = review_data[review_data['name'].isin(top_5)]\n","\n","# Create plot\n","plt.figure(figsize=(6, 4))\n","sns.countplot(\n","    y='name',\n","    hue='label',\n","    data=top_reviews,\n","    order=top_5,\n","    palette=['#1f77b4', '#5F9ED1'],  # Blue and orange\n","    edgecolor='black',\n","    linewidth=0.5\n",")\n","\n","# Format the plot (e.g., title, xlabel,legend)\n","plt.title('Useful vs Non Useful Reviews (Top 5 Businesses)')\n","plt.xlabel('Number of Reviews')\n","plt.ylabel('')\n","plt.legend(['Not Useful', 'Useful'], title='Review Type')\n","plt.grid(axis='x', alpha=0.2)\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"IivUiMgy110G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Histogram to display the distribution of review length for useful and not-useful reviews\n","plt.figure(figsize=(12, 5))\n","sns.histplot(\n","    x='review_length',\n","    hue='label',\n","    data=review_data,\n","    bins=50,\n","    palette=['#1f77b4', '#4F9ED1'],\n","    alpha=0.6,\n","    element='step'\n",")\n","plt.title('Review Length Distribution by Usefulness', pad=15)\n","plt.xlabel('Review Length')\n","plt.ylabel('Count')\n","plt.legend(['Not Useful', 'Useful'])\n","plt.xlim(0, 2000)  # Adjust based on your data\n","plt.grid(alpha=0.3)\n","plt.show()"],"metadata":{"id":"24CRzpZu50Wg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Bivariate"],"metadata":{"id":"EvvrA-zgsIGx"}},{"cell_type":"code","source":["# Review length by star rating\n","g = sns.FacetGrid(review_data, col='stars_review', height=3, aspect=0.8)\n","g.map(plt.hist, 'review_length', bins=50, color='#006BA4', edgecolor='white')\n","g.set_axis_labels('Review Length', 'Count')\n","g.set_titles('Star Rating: {col_name}')\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"HTm1sPLCJwi1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create cross-tabulation\n","star_counts = pd.crosstab(review_data['stars_review'], review_data['label'])\n","\n","# Plot heatmap\n","plt.figure(figsize=(6, 6))\n","sns.heatmap(star_counts, annot=True, fmt='d', cmap='YlGnBu',\n","            cbar_kws={'label': 'Number of Reviews'})\n","plt.title('Star Rating Counts by Usefulness')\n","plt.xlabel('Usefulness Label (0=Not Useful, 1=Useful)')\n","plt.ylabel('Star Rating')\n","plt.yticks(rotation=0)\n","plt.show()"],"metadata":{"id":"gLqveHyr46y3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# KDE of useful votes for popular vs non-popular businesses\n","plt.figure(figsize=(8, 5))\n","sns.kdeplot(data=review_data, x='useful', hue='is_popular_business',\n","            fill=True, common_norm=False, alpha=0.5, palette='GnBu')\n","plt.title('Distribution of Useful Votes by Business Popularity')\n","plt.xlabel('Useful Votes')\n","plt.legend(title='Popular Business?', labels=['No', 'Yes'])"],"metadata":{"id":"veFtPwX723BT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Scatter plot with regression line to display the relationship between useful and word count\n","plt.figure(figsize=(10, 5))\n","sns.regplot(x='word_count', y='useful', data=review_data,\n","            scatter_kws={'alpha':0.2}, line_kws={'color':'blue'})\n","plt.title('Word Count vs Useful Votes')\n","plt.xlabel('Word Count')\n","plt.ylabel('Useful Votes')"],"metadata":{"id":"PNiG3ZV03Ava"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Statistical Analysis"],"metadata":{"id":"jkFdvl12PHfR"}},{"cell_type":"code","source":["import scipy.stats as stats"],"metadata":{"id":"zVpPo_mgPHGe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate correlation coefficient and p-value\n","corr_coef, p_value = stats.pearsonr(review_data['stars_review'], review_data['stars_business'])\n","\n","# Print results\n","print(f\"Correlation Coefficient: {corr_coef:.2f}\")\n","print(f\"P-value: {p_value:.4f}\")\n","\n","# Create heatmap\n","corr = review_data[['stars_review', 'stars_business']].corr()\n","sns.heatmap(corr, annot=True, cmap='GnBu')\n","plt.title('Correlation between Customer Rating vs. Business Rating')\n","plt.show()"],"metadata":{"id":"DT4HXdjr5xp6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate Spearman rank correlation coefficient\n","corr_coef, p_value = stats.spearmanr(review_data['review_count'], review_data['stars_business'])\n","print(f\"Spearman Correlation (review_count vs stars_business): {corr_coef:.4f}, p-value: {p_value:.4f}\")"],"metadata":{"id":"S9-JAclCPZIu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Word Cloud\n"],"metadata":{"id":"LHgQT_cRQKy5"}},{"cell_type":"code","source":["# For the word cloud, remove short words(i.e., words that have 2 letters or less)\n","def remove_short_words(text):\n","  return ' '.join([word for word in text.split() if len(word)>2])\n","\n","review_data['filtered_text'] = review_data['cleaned_text'].apply(remove_short_words)"],"metadata":{"id":"IOhg51H8QQMW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1-star word cloud\n","one_star_text = ' '.join(review_data[review_data['stars_review'] == 1]['filtered_text'])\n","wordcloud = WordCloud(width=800, height=800, background_color='white', min_font_size=10).generate(one_star_text)\n","\n","plt.figure(figsize=(8,8))\n","plt.imshow(wordcloud)\n","plt.axis('off')\n","plt.title('1-Star Reviews Word Cloud')\n","plt.show()"],"metadata":{"id":"okG0zJQvQSzh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 5-star word cloud\n","five_star_text = ' '.join(review_data[review_data['stars_review'] == 5]['filtered_text'])\n","wordcloud = WordCloud(width=800, height=800, background_color='white', min_font_size=10).generate(five_star_text)\n","\n","plt.figure(figsize=(8,8))\n","plt.imshow(wordcloud)\n","plt.axis('off')\n","plt.title('5-Star Reviews Word Cloud')\n","plt.show()"],"metadata":{"id":"G9-m9Lb0QYRJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Bert"],"metadata":{"id":"4gP7H8odanXJ"}},{"cell_type":"code","source":["resampled_df['text'] = resampled_df['text'].apply(clean_text)  # Clean text"],"metadata":{"id":"_VwKhii5b82j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["useful_review_df = resampled_df[['text', 'label']]\n","useful_review_df.head()"],"metadata":{"id":"xRPXFhJXbgER"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import train test split 80% training/20% test\n","from sklearn.model_selection import train_test_split\n","train_df, test_df = train_test_split(useful_review_df, test_size=0.2, random_state=42)"],"metadata":{"id":"y7E7rT13aixr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import DistilBertTokenizerFast\n","\n","# Load the tokenizer\n","tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n","\n","# Tokenize the data\n","train_encodings = tokenizer(train_df['text'].tolist(), truncation=True, padding=True, max_length=128)\n","test_encodings = tokenizer(test_df['text'].tolist(), truncation=True, padding=True, max_length=128)"],"metadata":{"id":"QzvHsYOrbzl1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install transformers"],"metadata":{"collapsed":true,"id":"ZiuiKEYzb2m6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install torch"],"metadata":{"collapsed":true,"id":"QgAAQD2fZAUs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install datasets"],"metadata":{"collapsed":true,"id":"z_HJJ59vZDil"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","\n","# Define a custom Dataset class for the review data.\n","class ReviewDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","  # Initialize the dataset with input data and the target output and labels\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","      # Retrieve a single data sample at the given index 'idx'.\n","      # Convert encoding (input data) to pytorch tensors.\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","      # Add the corresponding label to the 'item' dictionary.\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","      # Return the total number of samples in the dataset (i.e.,the length of labels).\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","# wrap tokenized training data and labels into a pytorch dataset\n","train_dataset = ReviewDataset(train_encodings, train_df['label'].tolist())\n","# wrap tokenized test data and labels into a pytorch dataset\n","test_dataset = ReviewDataset(test_encodings, test_df['label'].tolist())"],"metadata":{"id":"tSoAjt9scay3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the DistilBERT model for sequence classification tasks.\n","from transformers import DistilBertForSequenceClassification\n","\n","# Load the model with 3 output labels\n","model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)"],"metadata":{"id":"JdXMKeUGcdvh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import TrainingArguments, EarlyStoppingCallback\n","\n","# Define the training arguments for fine-tuning the Hugging Face Transformer model.\n","training_args = TrainingArguments(\n","    output_dir='./results',  # Directory for checkpoint and output.\n","    num_train_epochs=5,  # Number of complete passes through the training dataset.\n","    per_device_train_batch_size=16,  # Batch size for training on each device (e.g., GPU/CPU).\n","    per_device_eval_batch_size=16,  # Batch size for evaluation on each device.\n","    warmup_steps=500,  # Number of steps for learning rate warm-up (gradual increase).\n","    weight_decay=0.2,  # L2 regularization to prevent overfitting.\n","    logging_dir='./logs',\n","    logging_steps=500,  # Log metrics and loss every 500 steps during training.\n","    eval_strategy=\"epoch\",  # Perform evaluation at the end of every epoch.\n","    save_strategy=\"epoch\",  # Save model checkpoints at the end of every epoch.\n","    load_best_model_at_end=True,  # Automatically load the best model (based on evaluation) after training finishes.\n","    metric_for_best_model=\"eval_loss\",  # Use validation loss to determine the best model.\n","    greater_is_better=False,  # Indicate that lower validation loss is better.\n","    report_to='none'  # Avoid reporting metrics to external systems (e.g., WandB).\n",")\n"],"metadata":{"id":"X-6dXKL2ciqg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import the necessary libraries\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n","\n","# function to compute metrics\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","\n","    # Calculate performance metrics\n","    accuracy = accuracy_score(labels, predictions)\n","    precision = precision_score(labels, predictions, average='macro')\n","    recall = recall_score(labels, predictions, average='macro')\n","    f1 = f1_score(labels, predictions, average='macro')\n","\n","# Return specified metrcis\n","    return {\n","        'accuracy': accuracy,\n","        'precision': precision,\n","        'recall': recall,\n","        'f1': f1\n","    }"],"metadata":{"id":"HwgQu-fkcqe6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train the trainer using the defined variables\n","from transformers import Trainer\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=test_dataset,\n","    compute_metrics=compute_metrics,\n","    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],  # Stop after 3 epochs of no improvement\n",")\n","\n","trainer.train()"],"metadata":{"id":"xPu7i8ogcu2S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import necessary libraries\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.svm import SVC\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import GridSearchCV, train_test_split\n","from sklearn.metrics import accuracy_score, classification_report, f1_score"],"metadata":{"id":"D3UF-QDAegeB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#train_df, test_df = train_test_split(useful_review_df, test_size=0.2, random_state=42)"],"metadata":{"id":"lwd97tNJc_kr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the TfidfVectorizer and Naive Bayes model within a pipeline\n","nb_pipeline = Pipeline([\n","    ('tfidf', TfidfVectorizer()),  # perfrom tf-idf vectorization\n","    ('nb', MultinomialNB())        # instance for multinomial naive bayes classifier\n","])\n","\n","# Define parameter grid for hyperparameter tuning\n","param_grid = {\n","    'tfidf__max_features': [5000, 10000],       # Vocabulary size for TF-IDF\n","    'tfidf__ngram_range': [(1, 1), (1, 2)],     # Unigrams and bigrams\n","    'tfidf__stop_words': [None, 'english'],     # Include or exclude stopwords\n","    'tfidf__min_df': [1, 3],                   # Minimum document frequency\n","    'tfidf__max_df': [0.7, 1.0],               # Maximum document frequency\n","    'nb__alpha': [0.1, 0.5]               # Smoothing parameter for MultinomialNB\n","}"],"metadata":{"id":"zTqLaWQWdDfY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#  GridSearchCV to find the best hyperparameters\n","grid_search = GridSearchCV(nb_pipeline, param_grid, cv=3, scoring='f1_weighted', n_jobs=-1)\n","grid_search.fit(train_df['text'], train_df['label'])"],"metadata":{"id":"wVEBM4bSdF2_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get the best model from grid search\n","best_model = grid_search.best_estimator_\n","\n","# Print the best hyperparameters\n","print(\"Best Parameters:\", grid_search.best_params_)"],"metadata":{"id":"pf5wvr2pdIvm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate the model on the test set\n","y_pred = best_model.predict(test_df['text'])\n","accuracy = accuracy_score(test_df['label'], y_pred)\n","f1 = f1_score(test_df['label'], y_pred, average='weighted')\n","\n","print(\"Accuracy:\", accuracy)\n","print(\"F1 Score:\", f1)\n","print(\"Classification Report:\\n\", classification_report(test_df['label'], y_pred))"],"metadata":{"id":"6cp3cjkbdM8c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# SVM with RBF kernel model\n","svm_pipeline = Pipeline([\n","    ('tfidf', TfidfVectorizer(max_features=5000, ngram_range=(1, 2), min_df=3, max_df=0.7)),\n","    ('svm_rbf', SVC(C=1.0, kernel='rbf', gamma='scale'))\n","])\n","\n","# Fit the model with raw text data\n","svm_pipeline.fit(train_df['text'], train_df['label'])"],"metadata":{"id":"RocVM8Z9dk8Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Predict the labels on the test dataset\n","# Use the raw text data from the test set (test_df['text'])\n","predictions_svm_rbf = svm_pipeline.predict(test_df['text'])\n","\n","# Evaluate the model performance\n","accuracy = accuracy_score(test_df['label'], predictions_svm_rbf)\n","print(\"Accuracy \", accuracy * 100)\n","\n","# Print  Report\n","print(\"Classification Report:\\n\", classification_report(test_df['label'], predictions_svm_rbf))"],"metadata":{"id":"6wANUeWXdyAl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#test_df.shape"],"metadata":{"id":"D5dzFgmv3zK4"},"execution_count":null,"outputs":[]}]}